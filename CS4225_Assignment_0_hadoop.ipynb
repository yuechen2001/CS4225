{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuechen2001/CS4225/blob/master/CS4225_Assignment_0_hadoop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS4225 Assignment 0: Hadoop Streaming Warm-up (Token Counting)\n",
        "\n",
        "Assignment 0 is a guided warm-up to help you get comfortable with **Hadoop Streaming** on Colab. You will implement a simple mapper/reducer that counts tokens in a text stream. You do **not** need to submit anything for this notebook, but the mechanics here will be used in Assignment 1.\n"
      ],
      "metadata": {
        "id": "1zlFq4CB54IV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9bT9M1yvyXG"
      },
      "source": [
        "# Installing Hadoop\n",
        "\n",
        "First, run the following cell, which downloads and installs Hadoop to the `usr/local` directory, and then sets the `JAVA_HOME` environment variable so Hadoop knows where to find Java on our system.\n",
        "\n",
        "To run the cell, either click the \"play\" icon at the top left of the cell, or use the keyboard shortcut Ctrl/âŒ˜+Enter, or Shift+Enter to run and go to the next cell. The installation process should take about 45 seconds or so. Note that if you refresh the colab notebook, you need to run this cell again before you can use Hadoop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bijZAdD_cBMK"
      },
      "outputs": [],
      "source": [
        "!wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz\n",
        "!tar -xzf hadoop-3.3.5.tar.gz\n",
        "# install hadoop to /usr/local\n",
        "!cp -r hadoop-3.3.5/ /usr/local/\n",
        "import os\n",
        "# hadoop needs to know where java is located\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "# the following line reduces the amount of Hadoop's printed output, to make it easier to see your own debug output\n",
        "os.environ[\"HADOOP_ROOT_LOGGER\"] = \"WARN,console\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj00rPPZyEWZ"
      },
      "source": [
        "# Downloading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgEGfCcGOswd"
      },
      "outputs": [],
      "source": [
        "!wget https://nuojohnchen.github.io/teaching/cs4225/assign0/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hadoop Streaming\n",
        "\n",
        "The `%%file` command in colab means that when we execute this block, colab will write the Python program in the block to a file\n",
        "located at `/content/mapper.py` (note the slash in front). Generally\n",
        "in colab, `/content` is the default directory where your files are stored.\n",
        "\n",
        "It is not recommended to code your mapper and reducer directly into\n",
        "the files `/content/mapper.py` and `/content/reducer.py`; this is quite unsafe, as colab\n",
        "does not in general save these files, and when you re-open the notebook\n",
        "later they will most likely be gone. It is better to follow the below approach and code them into\n",
        "notebook cells, and then use the `%%file` command.\n",
        "\n",
        "**Mapper:** Like in regular Hadoop, the framework divides the input file into splits (e.g., 128MB) and passes them to different mappers. But unlike regular Hadoop where the mappers receive their input as key-value pairs, here the mapper just takes in lines of text (which are lines from the input file). Instead of an explicit `map()` function, here we just iterate over the lines of text and process them one by one. Instead of emitting key-value pairs like in regular Hadoop, here the mapper emits tab-separated lines of plain text of the form \"`key\\tvalue`\". Both the mapper and reducer read their inputs from `sys.stdin`, and write their outputs to `sys.stdout`."
      ],
      "metadata": {
        "id": "HyK72gcdLtvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file mapper.py\n",
        "\n",
        "import io\n",
        "import sys\n",
        "input_stream = io.TextIOWrapper(sys.stdin.buffer, encoding='latin1')\n",
        "\n",
        "for line in sys.stdin:\n",
        "\n",
        "  # Here we split each line of the incoming input, and emit each word, with\n",
        "  # its count of 1, separated by a tab. This corresponds to the map() function\n",
        "  # in Hadoop.\n",
        "  for word in line.split():\n",
        "    print(f\"{word}\\t1\")"
      ],
      "metadata": {
        "id": "WrKizsCsG5yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reducer:** Like in regular\n",
        "Hadoop, the data emitted by the mappers is grouped by key and sorted, and\n",
        "then passed to the reducer responsible for that key. But unlike regular Hadoop\n",
        "where the `reduce()` function's input is in the form `<key, List[values]>`,\n",
        "here the reducer receives its input line by line, in the same format emitted\n",
        "by the mappers."
      ],
      "metadata": {
        "id": "05_VjPd-LVZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file reducer.py\n",
        "\n",
        "import sys\n",
        "\n",
        "# Initialization: here we create the data structures we need; this corresponds\n",
        "# to the setup() function in Hadoop.a\n",
        "counts = {}\n",
        "\n",
        "for line in sys.stdin:\n",
        "\n",
        "  # Here we process each line of the incoming input; this corresponds to the\n",
        "  # reduce() function in Hadoop.\n",
        "  word, count = line.strip().split('\\t')\n",
        "  if word not in counts:\n",
        "    counts[word] = 0\n",
        "  counts[word] += int(count)\n",
        "\n",
        "# Postprocess: after processing all lines, we do any necessary post-processing,\n",
        "# corresponding to the cleanup() function in Hadoop.\n",
        "for word, count in counts.items():\n",
        "  print(f\"{word}\\t{count}\")"
      ],
      "metadata": {
        "id": "twObdFyKzkza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu5IAGT2Os6D"
      },
      "outputs": [],
      "source": [
        "# Set permissions to ensure that Hadoop can use the files\n",
        "!chmod u+rwx /content/mapper.py\n",
        "!chmod u+rwx /content/reducer.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Hadoop Streaming:** the first line deletes the `/content/output` folder to prevent errors when running the program multiple times."
      ],
      "metadata": {
        "id": "aw6QtYKUTrt9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru1lPx9yOsvO"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/output\n",
        "!/usr/local/hadoop-3.3.5/bin/hadoop jar /usr/local/hadoop-3.3.5/share/hadoop/tools/lib/hadoop-streaming-3.3.5.jar \\\n",
        "-input /content/input.txt \\\n",
        "-output /content/output \\\n",
        "-file /content/mapper.py \\\n",
        "-file /content/reducer.py \\\n",
        "-mapper 'python mapper.py' \\\n",
        "-reducer 'python reducer.py'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is stored in the `/content/output/part-00000` file. (If we had multiple reducers, there would be multiple files, but for our assignments it is sufficient to use just 1 reducer (and Hadoop's default behavior is to use just 1 reducer)."
      ],
      "metadata": {
        "id": "zf2E-5jiWdAv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTFjOm59kc04"
      },
      "outputs": [],
      "source": [
        "!cat /content/output/part-00000"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d1Ar4KYntosv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}